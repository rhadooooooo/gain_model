import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd

torch.autograd.set_detect_anomaly(True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ----------------------------------
# Simplified GAIN cu pierdere adversariala BCE si de reconstructie
# ----------------------------------
class GAIN(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        # Generator: primește datele incomplete + hint
        self.gen = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            # strat ascuns suplimentar
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim, input_dim),
            nn.Tanh()
        )

        # Discriminator: primește datele completate + hint
        self.disc = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            # strat ascuns suplimentar
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    def forward(self, x_hat, mask, hint):
        # Generator forward
        gen_input = torch.cat([x_hat, hint], dim=1)           #concateneaza x_hat si hint pe axa de caracteristici
        generated = self.gen(gen_input)                       #genereaza valorile lipsa
        imputed = mask * x_hat + (1 - mask) * generated       #pastreaza valorile existente si inlocuieste missing
        # Discriminator forward
        disc_input = torch.cat([imputed, hint], dim=1)        #concateneaza imputarea si hint-ul
        d_prob = self.disc(disc_input)                        #calculeaza probabilitatea pentru fiecare caracteristica
        return generated, imputed, d_prob                     #returneaza: generator output, imputare, probabilitati

# ----------------------------------
# Normalize / Denormalize (Min-Max to [-1,1])
# ----------------------------------
def normalize(data):
    dmin = np.nanmin(data, axis=0)                  #calculeaza minimul pe fiecare coloana
    dmax = np.nanmax(data, axis=0)                  #calculeaza maximul pe fiecare coloana
    span = dmax - dmin;                             #intervalul original (max-min) pentru fiecare coloana 
    span[span==0] = 1                               #evita impartirea la zero daca max==min
    scaled = 2 * (data - dmin) / span - 1           #re-scaleaza datele in intervalul [-1.1]
    return scaled, dmin, dmax                       #returneaza datele normalizate si parametrii pentru denormalizare

def denormalize(data, dmin, dmax):                  #inverseaza normalizarea: din [-1,1] -> [min,max] original
    return (data + 1) / 2 * (dmax - dmin) + dmin

# ----------------------------------
# Incarcare date si pregatire tensori
# ----------------------------------
df = pd.read_csv("diabetes_readmission_NAN.csv")                  #citeste tabelul cu date incomplete (NAN)
mask_df = pd.read_csv("diabetes_readmission_NAN_to_mask.csv")     #citeste masca (1 pentru date prezente, 0 pentru lipsa)
true_df = pd.read_csv("diabetes_readmission.csv")                 #citeste tabelul complet cu date reale pentru calculul pierderii pe missing

data_np = df.values.astype(np.float32)
mask_np = mask_df.values.astype(np.float32)
true_np = true_df.values.astype(np.float32)

# normalize and fill NaN with 0
norm_np, dmin, dmax = normalize(data_np)
norm_np[np.isnan(norm_np)] = 0
norm_true, _, _ = normalize(true_np)
norm_true[np.isnan(norm_true)] = 0

x = torch.tensor(norm_np, device=device)
m = torch.tensor(mask_np, device=device)
true_x = torch.tensor(norm_true, device=device)

# ----------------------------------
# Parametrii si initializarea modelului
# ----------------------------------
input_dim = x.size(1)
model = GAIN(input_dim).to(device)     #creeaza modelul GAIN si il muta pe GPU/CPU
optimizer_G = optim.Adam(model.gen.parameters(), lr=5e-4, betas=(0.5,0.999))    #rata de invatare generator
optimizer_D = optim.Adam(model.disc.parameters(), lr=1e-4, betas=(0.5,0.999))   #rata de invatare discriminator
bce = nn.BCELoss(reduction='none')     #functia de BCE fara mediere interna
num_epochs = 100                       #numarul total de iteratii
lambda_recon = 200     #pondere pierdere reconstructie pe valori observate          
lambda_miss = 1500     #pondere pierdere reconstructie pe valori lipsa

batch_size = x.size(0)

# ----------------------------------
# ANTRENARE
# ----------------------------------
for epoch in range(1, num_epochs+1):
    # annealing liniar de la 1.0 → 0.1
    p_hint = 1.0 - 0.9 * (epoch-1)/(num_epochs-1)   #calculeaza probabilitatea hint-ului care scade liniar
    # --- Generare hint ---
    rnd = torch.rand_like(m)  # generează tensor aleator cu aceleași dimensiuni ca masc
    hint = (rnd < p_hint).float() * m + 0.5 * (1 - (rnd < p_hint).float())

    # --- Forward generator + build imputed ---
    gen_input = torch.cat([x, hint], dim=1)  # concatenează intrările x și hint pe axa caracteristicilor
    generated = model.gen(gen_input)   # trece prin generator și generează valori pentru missing
    imputed = m * x + (1 - m) * generated   # combină valorile reale cu cele generate

    # --- Discriminator update ---
    real_input = torch.cat([x, hint], dim=1)   # intrarea reală pentru discriminator
    d_real = model.disc(real_input)            # scorul discriminatorului pe date reale
    fake_input = torch.cat([imputed.detach(), hint], dim=1)   # intrarea fake fără gradent către G
    d_fake = model.disc(fake_input)   # scorul discriminatorului pe date generate                    

    loss_D = ( m * bce(d_real, torch.ones_like(d_real))
             + (1-m) * bce(d_fake, torch.zeros_like(d_fake)) ).mean()    # pierdere BCE: real→1 , fake→0
    
    optimizer_D.zero_grad()   #reseteaza gradientele discriminatorului
    loss_D.backward()        #calculeaza gradientele pentru discriminator
    optimizer_D.step()        #actualizeaza parametrii discriminator

    # --- Generator update ---
    fake_input_forG = torch.cat([imputed, hint], dim=1)   # intrarea fake cu gradient pentru G
    d_fake_forG = model.disc(fake_input_forG)        # scorul discriminatorului pe fake (cu grad)

    adv_loss = ((1-m) * bce(d_fake_forG, torch.ones_like(d_fake_forG))).mean()     # pierdere adversarială: vrem fake→1
    rec_loss = (m * (generated - x)**2).mean()       # pierdere MSE pe pozițiile observate
    # reconstruction on missing using ground-truth
    rec_miss = ((1-m) * (generated - true_x)**2).mean()       # pierdere MSE pe pozițiile lipsă

    loss_G = adv_loss \
           + lambda_recon * rec_loss \
           + lambda_miss * rec_miss           # sumă ponderată a tuturor pierderilor
    
    optimizer_G.zero_grad()
    loss_G.backward() 
    optimizer_G.step()

    # --- RMSE_missing & log ---
    mse_miss = ((1-m) * (generated - true_x)**2).sum()/(1-m).sum()
    rmse = torch.sqrt(mse_miss)
    print(f"Epoch {epoch}/{num_epochs} | D_loss={loss_D:.4f} | G_loss={loss_G:.4f} | RMSE_missing={rmse:.4f}")

# ----------------------------------
# Save imputations
# ----------------------------------
with torch.no_grad():
    _, imp_final, _ = model(x, m, torch.zeros_like(m))
    imp_np = imp_final.cpu().numpy()
    final_np = denormalize(imp_np, dmin, dmax)
    pd.DataFrame(final_np, columns=df.columns).to_csv(
        "diabetes_readmission_imputed_with_missing_loss_lambda_1500_Tanh_strat_suplimentar_lr_5e_4.csv", index=False)
print("Training complete and results saved.")
