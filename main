import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd

torch.autograd.set_detect_anomaly(True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ----------------------------------
# Simplified GAIN cu pierdere adversariala BCE si de reconstructie
# ----------------------------------
class GAIN(nn.Module):
    def __init__(self, input_dim, hidden_dim=512, p_dropout=0.05):
        super().__init__()
        # Generator: 
        self.gen = nn.Sequential(
            nn.Linear(input_dim*2, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, input_dim),
            nn.Tanh()
        )
        # Discriminator:
        self.disc = nn.Sequential(
            nn.Linear(input_dim*2, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(p_dropout),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x_hat, mask, hint):
        # generator
        gen_input = torch.cat([x_hat, hint], dim=1)
        generated = self.gen(gen_input)
        # imputed: pastreaza valorile reale si inlocuieste missing
        imputed = mask * x_hat + (1 - mask) * generated
        # discriminator
        disc_input = torch.cat([imputed, hint], dim=1)
        d_prob = self.disc(disc_input)
        return generated, imputed, d_prob

# ----------------------------------
# Normalize / Denormalize (Min-Max to [-1,1])
# ----------------------------------
def normalize(data):
    dmin = np.nanmin(data, axis=0)
    dmax = np.nanmax(data, axis=0)
    span = dmax - dmin; span[span==0] = 1
    scaled = 2 * (data - dmin) / span - 1
    return scaled, dmin, dmax

def denormalize(data, dmin, dmax):
    return (data + 1) / 2 * (dmax - dmin) + dmin

# ----------------------------------
# Incarcare date si pregatire tensori
# ----------------------------------
df      = pd.read_csv("diabetes_readmission_NAN.csv")
mask_df = pd.read_csv("diabetes_readmission_NAN_to_mask.csv")
true_df = pd.read_csv("diabetes_readmission.csv")

data_np = df.values.astype(np.float32)
mask_np = mask_df.values.astype(np.float32)
true_np = true_df.values.astype(np.float32)

# normalize & fill NaN with 0
norm_np, dmin, dmax   = normalize(data_np)
norm_np[np.isnan(norm_np)] = 0
norm_true, _, _       = normalize(true_np)
norm_true[np.isnan(norm_true)] = 0

# compute per-column mean & std only on observed values
mu = np.zeros(norm_np.shape[1], dtype=np.float32)
sigma = np.zeros(norm_np.shape[1], dtype=np.float32)
for j in range(norm_np.shape[1]):
    obs = norm_np[mask_np[:,j]==1, j]
    mu[j]    = np.nanmean(obs)
    sigma[j] = np.nanstd(obs) if np.nanstd(obs)>0 else 1.0


x      = torch.tensor(norm_np, device=device)
m      = torch.tensor(mask_np, device=device)
true_x = torch.tensor(norm_true, device=device)
mu_t   = torch.tensor(mu,    device=device).unsqueeze(0)  # shape [1, input_dim]
sigma_t= torch.tensor(sigma, device=device).unsqueeze(0)

# ----------------------------------
# Parametrii & initializarea modelului
# ----------------------------------
input_dim   = x.size(1)
model       = GAIN(input_dim, hidden_dim=512).to(device)
optimizer_G = optim.Adam(model.gen.parameters(),  lr=1e-4, betas=(0.5,0.999))
optimizer_D = optim.Adam(model.disc.parameters(), lr=5e-5, betas=(0.5,0.999))
bce         = nn.BCELoss(reduction='none')
num_epochs  = 400
warm_epochs  = 20
lambda_recon= 500
lambda_miss = 1000

# ----------------------------------
# Warm-start: pre-antrenează generator doar pe rec_miss
# ----------------------------------
for ws in range(1, warm_epochs+1):
    # sample static Gaussian noise z pentru missing
    z = torch.randn_like(x) * sigma_t + mu_t
    x_hat = m * x + (1-m) * z
    # forward only generator
    gen_input = torch.cat([x_hat, torch.zeros_like(m)], dim=1)
    generated = model.gen(gen_input)
    rec_miss = ((1-m) * (generated - true_x)**2).mean()
    optimizer_G.zero_grad()
    rec_miss.backward()
    optimizer_G.step()
    print(f"Warm-start {ws}/{warm_epochs} | rec_miss={rec_miss:.4f}")

# ----------------------------------
# ANTRENARE
# ----------------------------------
for epoch in range(1, num_epochs+1):
    # annealing hint
    p_hint = 1.0 - 0.9 * (epoch-1)/(num_epochs-1)
    rnd    = torch.rand_like(m)
    hint   = (rnd < p_hint).float() * m + 0.5 * (1 - (rnd < p_hint).float())

    # --- generează zgomot gaussian static pentru missing ---
    z = torch.randn_like(x) * sigma_t + mu_t

    # MSE și RMSE doar pentru imputarea statică (z) pe pozițiile lipsă
    mse_static  = ((1-m) * (z - true_x)**2).sum() / (1-m).sum()
    rmse_static = torch.sqrt(mse_static)

    # construit x_hat cu zgomotul static și forward prin generator
    x_hat      = m * x + (1 - m) * z
    generated, imputed, _ = model(x_hat, m, hint)

    # --- Discriminator update ---
    d_real = model.disc(torch.cat([m*x + (1-m)*z, hint], dim=1))
    d_fake = model.disc(torch.cat([imputed.detach(), hint], dim=1))
    loss_D = ( m * bce(d_real, torch.ones_like(d_real))
             + (1-m) * bce(d_fake, torch.zeros_like(d_fake)) ).mean()
    optimizer_D.zero_grad(); loss_D.backward(); optimizer_D.step()

    # --- Generator update ---
    d_fake_forG = model.disc(torch.cat([imputed, hint], dim=1))
    adv_loss  = ((1-m) * bce(d_fake_forG, torch.ones_like(d_fake_forG))).mean()
    rec_loss  = (m * (generated - x)**2).mean()
    rec_miss  = ((1-m) * (generated - true_x)**2).mean()
    loss_G    = adv_loss + lambda_recon*rec_loss + lambda_miss*rec_miss
    optimizer_G.zero_grad(); loss_G.backward(); optimizer_G.step()

    # --- metrici GAIN ---
    mse_gain   = ((1-m) * (generated - true_x)**2).sum()/(1-m).sum()
    rmse_gain  = torch.sqrt(mse_gain)
    rmse_glob  = torch.sqrt(((imputed - true_x)**2).mean())

    print(
        f"Epoch {epoch}/{num_epochs} | "
        f"D_loss={loss_D:.4f} | G_loss={loss_G:.4f} | "
        f"RMSE_static={rmse_static:.4f} | "
        f"RMSE_missing_GAIN={rmse_gain:.4f} | "
        f"RMSE_global={rmse_glob:.4f}"
    )

# ----------------------------------
# Salvare rezultate
# ----------------------------------
with torch.no_grad():
    _, imp_final, _ = model(x, m, torch.zeros_like(m))
    imp_np = imp_final.cpu().numpy()
    final_np = denormalize(imp_np, dmin, dmax)
    pd.DataFrame(final_np, columns=df.columns).to_csv(
        "diabetes_readmission_imputed_static_gauss_warm_start_G_lr_1e-4_epochs_400_lrcon_500_lmiss_1000_h_dim_512_p_drop_005_RMSE_static_.csv", index=False)

print("Antrenare completa si salvata!.")
